# -*- coding: utf-8 -*-
"""Clustering of Customer Segmentation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pFzaiBT7MZOQvoeycka1jOA1asBXKZ0r
"""

# Step 1: Install ydata-profiling
!pip install ydata-profiling openpyxl

# Step 2: Import libraries
import pandas as pd
from ydata_profiling import ProfileReport
from google.colab import files

# Step 4: Load dataset
# Replace 'your_file.xlsx' with the exact filename after upload
df = pd.read_csv("/content/shopping_behavior_updated.csv")

# Step 5: Generate profile report
profile = ProfileReport(df, title="Customer Segmentation EDA Report", explorative=True)

# Step 6A: Display report inside Colab
profile.to_notebook_iframe()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, silhouette_score

# Drop ID-like columns if present
df = df.drop(columns=['Customer_ID'], errors='ignore')

numeric_features = ['Age', 'Purchase Amount (USD)', 'Review Rating', 'Previous Purchases']
categorical_features = ['Gender', 'Item Purchased', 'Category', 'Location', 'Size', 'Color',
                        'Season', 'Subscription Status', 'Shipping Type', 'Discount Applied',
                        'Promo Code Used', 'Payment Method', 'Frequency of Purchases']

# Preprocessing pipelines
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

# Full preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

inertia = []
K = range(2, 11)  # test clusters from 2 to 10

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_processed)
    inertia.append(kmeans.inertia_)

# Plot elbow curve
plt.figure(figsize=(8,5))
plt.plot(K, inertia, 'bo-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (WCSS)')
plt.title('Elbow Method for Optimal k')
plt.show()

from sklearn.metrics import silhouette_score

for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_processed)
    score = silhouette_score(X_processed, labels)
    print(f"k={k}, Silhouette Score={score:.3f}")

# ---------------------------
# STEP 2: Feature Engineering
# ---------------------------
from sklearn.preprocessing import StandardScaler

# Select only numeric features for clustering
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns

X = df[numeric_cols]

# Standardize the data (important for K-Means)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ---------------------------
# STEP 3: K-Means Segmentation
# ---------------------------
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Find the optimal k using Elbow Method
inertia = []
K = range(1, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

# Plot Elbow Curve
plt.figure(figsize=(8, 5))
plt.plot(K, inertia, 'bo-')
plt.xlabel("Number of clusters (k)")
plt.ylabel("Inertia (SSE)")
plt.title("Elbow Method For Optimal k")
plt.show()

# Fit KMeans with chosen k
final_k = 3  # try 2 and 4 as well
kmeans = KMeans(n_clusters=final_k, random_state=42, n_init=10)
df['Cluster'] = kmeans.fit_predict(X_scaled)

# View cluster sizes
print(df['Cluster'].value_counts())

# Select only the required columns
numeric_cols = ['Age', 'Purchase Amount (USD)']
categorical_cols = ['Gender', 'Category', 'Item Purchased']

# Numeric features: cluster means
numeric_summary = df.groupby('Cluster')[numeric_cols].mean()

# Categorical features: cluster modes
categorical_summary = df.groupby('Cluster')[categorical_cols].agg(lambda x: x.value_counts().index[0])

# Combine numeric + categorical summaries
cluster_profile = pd.concat([numeric_summary, categorical_summary], axis=1)

cluster_profile

from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# Assuming your data is X (scaled numeric features used for KMeans)
labels = kmeans.labels_

# 1. Silhouette Score: [-1, 1] → higher = better (close to 1 means well-clustered)
silhouette = silhouette_score(X, labels)

# 2. Davies-Bouldin Index: lower = better
db_index = davies_bouldin_score(X, labels)

# 3. Calinski-Harabasz Index: higher = better
ch_index = calinski_harabasz_score(X, labels)

print(f"Silhouette Score: {silhouette:.3f}")
print(f"Davies-Bouldin Index: {db_index:.3f}")
print(f"Calinski-Harabasz Index: {ch_index:.3f}")

"""**AGGLOMERATIVE CLUSTERING**"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# Select only needed features
features = ["Age", "Gender", "Category", "Item Purchased", "Purchase Amount (USD)"  ]
df_selected = df[features].copy()

# Encode categorical variables
le_dict = {}
for col in ["Gender", "Category", "Item Purchased"]:
    le = LabelEncoder()
    df_selected[col] = le.fit_transform(df_selected[col])
    le_dict[col] = le  # store encoders in case you want to inverse transform later

# Scale numeric variables
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_selected)

# Apply Agglomerative Clustering
agg = AgglomerativeClustering(n_clusters=4, linkage='ward')  # try 3 clusters first
df['Cluster'] = agg.fit_predict(df_scaled)

# Validation metrics
silhouette = silhouette_score(df_scaled, df['Cluster'])
db_index = davies_bouldin_score(df_scaled, df['Cluster'])
ch_index = calinski_harabasz_score(df_scaled, df['Cluster'])

print("Silhouette Score:", silhouette)
print("Davies-Bouldin Index:", db_index)
print("Calinski-Harabasz Index:", ch_index)

# Cluster profile summary
numeric_summary = df.groupby('Cluster')[["Age", "Purchase Amount (USD)"]].mean()
categorical_summary = df.groupby('Cluster')[["Gender", "Category", "Item Purchased"]].agg(lambda x: x.value_counts().index[0])
cluster_profile = pd.concat([numeric_summary, categorical_summary], axis=1)

print("\nCluster Profile:")
print(cluster_profile)

"""**DBSCAN**"""

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# Scale features (important for DBSCAN)
X_scaled = StandardScaler().fit_transform(X)  # X is your numeric feature matrix

# Run DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)  # eps = neighborhood radius, min_samples = min pts to form a cluster
labels = dbscan.fit_predict(X_scaled)

# Count clusters (ignoring noise)
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
print("Estimated number of clusters:", n_clusters)

# Evaluate only if we have >1 cluster
if n_clusters > 1:
    print("Silhouette Score:", silhouette_score(X_scaled, labels))
    print("Davies-Bouldin Index:", davies_bouldin_score(X_scaled, labels))
    print("Calinski-Harabasz Index:", calinski_harabasz_score(X_scaled, labels))

# Check cluster distribution
import pandas as pd
df["Cluster"] = labels
print(df.groupby("Cluster").mean())

# Numeric cluster means
numeric_cols = df.select_dtypes(include=['int64','float64']).columns
numeric_summary = df.groupby("Cluster")[numeric_cols].mean()

# Categorical cluster modes
categorical_cols = df.select_dtypes(include=['object']).columns
categorical_summary = df.groupby("Cluster")[categorical_cols].agg(lambda x: x.value_counts().index[0])

# Combine results
cluster_profile = pd.concat([numeric_summary, categorical_summary], axis=1)
cluster_profile

from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

X = df_scaled  # your normalized data

# Try with 2 clusters first
gmm = GaussianMixture(n_components=2, random_state=42)
clusters = gmm.fit_predict(X)

df["Cluster"] = clusters

# Evaluation
silhouette = silhouette_score(X, clusters)
db_index = davies_bouldin_score(X, clusters)
ch_index = calinski_harabasz_score(X, clusters)

print(f"Silhouette Score: {silhouette}")
print(f"Davies-Bouldin Index: {db_index}")
print(f"Calinski-Harabasz Index: {ch_index}")

"""**Taking all the col**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# ===== 1. Load Data =====
# replace with your actual dataset file
df = pd.read_csv("/content/shopping_behavior_updated.csv")

# ===== 2. Encode categorical variables =====
# Automatically encode all categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

# ===== 3. Scale the data =====
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)

# ===== 4. Run KMeans for multiple k =====
results = {}
for k in range(2, 7):  # you can expand range
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(scaled_data)

    sil = silhouette_score(scaled_data, labels)
    db = davies_bouldin_score(scaled_data, labels)
    ch = calinski_harabasz_score(scaled_data, labels)

    results[k] = {
        "Silhouette": sil,
        "Davies-Bouldin": db,
        "Calinski-Harabasz": ch
    }

# ===== 5. Display results =====
print("Cluster Evaluation Metrics:")
for k, metrics in results.items():
    print(f"\nK = {k}")
    print(f"  Silhouette Score: {metrics['Silhouette']:.4f}")
    print(f"  Davies-Bouldin Index: {metrics['Davies-Bouldin']:.4f}")
    print(f"  Calinski-Harabasz Index: {metrics['Calinski-Harabasz']:.4f}")

"""**PCA + K-means**"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

df = pd.read_csv("/content/shopping_behavior_updated.csv")

# 1️⃣ Convert categorical columns into numeric (One-Hot Encoding)
df_encoded = pd.get_dummies(df, drop_first=True)

# 2️⃣ Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_encoded)


# ✅ Apply PCA (reduce to 2 or 3 dimensions first, can try more later)
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_scaled)

print("Explained variance ratio:", pca.explained_variance_ratio_)
print("Total variance explained:", pca.explained_variance_ratio_.sum())

# ---------- KMeans after PCA ----------
kmeans = KMeans(n_clusters=3, random_state=42)
labels_kmeans = kmeans.fit_predict(X_pca)

print("\nKMeans after PCA:")
print("Silhouette Score:", silhouette_score(X_pca, labels_kmeans))
print("Davies-Bouldin Index:", davies_bouldin_score(X_pca, labels_kmeans))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X_pca, labels_kmeans))

# ---------- Agglomerative after PCA ----------
agg = AgglomerativeClustering(n_clusters=3)
labels_agg = agg.fit_predict(X_pca)

print("\nAgglomerative after PCA:")
print("Silhouette Score:", silhouette_score(X_pca, labels_agg))
print("Davies-Bouldin Index:", davies_bouldin_score(X_pca, labels_agg))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X_pca, labels_agg))

"""**UMAP + K-Means + HDBSCAN**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import umap.umap_ as umap
import hdbscan

# ✅ Copy df and encode categorical columns
df_encoded = df.copy()
for col in df_encoded.select_dtypes(include=['object']).columns:
    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col])

# ✅ Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_encoded)

# ✅ UMAP (nonlinear reduction to 2D)
umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
X_umap = umap_model.fit_transform(X_scaled)

print("UMAP shape:", X_umap.shape)

# ---------- KMEANS on UMAP ----------
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans_labels = kmeans.fit_predict(X_umap)

print("\nKMeans after UMAP:")
print("Silhouette Score:", silhouette_score(X_umap, kmeans_labels))
print("Davies-Bouldin Index:", davies_bouldin_score(X_umap, kmeans_labels))
print("Calinski-Harabasz Index:", calinski_harabasz_score(X_umap, kmeans_labels))

# ---------- HDBSCAN on UMAP ----------
hdb = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=10, metric='euclidean')
hdb_labels = hdb.fit_predict(X_umap)

# Filter noise points (-1) if exist
mask = hdb_labels != -1
if mask.sum() > 0:
    print("\nHDBSCAN after UMAP (excluding noise):")
    print("Silhouette Score:", silhouette_score(X_umap[mask], hdb_labels[mask]))
    print("Davies-Bouldin Index:", davies_bouldin_score(X_umap[mask], hdb_labels[mask]))
    print("Calinski-Harabasz Index:", calinski_harabasz_score(X_umap[mask], hdb_labels[mask]))
    print("Number of clusters found:", len(set(hdb_labels)) - (1 if -1 in hdb_labels else 0))
else:
    print("HDBSCAN found only noise points. Try smaller min_cluster_size or lower min_samples.")

import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(X_umap[:,0], X_umap[:,1], c=labels, cmap="Spectral", s=10)
plt.colorbar()
plt.title("UMAP + HDBSCAN Clusters")
plt.show()